{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "TODO:  describe choices \n",
    "\n",
    "* PEFT technique: LoRA\n",
    "* Model: gpt2\n",
    "* Evaluation approach: Huggingface Evaluate\n",
    "* Fine-tuning dataset:imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "TODO: In the cells below, we load chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f551c63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scikit-learn\n",
    "#from transformers import AutoModelForCausalLM\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling,DataCollatorWithPadding\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4935cb4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd116d4b62c74533968cc037c8c434d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/7.81k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 21.0M/21.0M [00:01<00:00, 20.5MB/s]\n",
      "Downloading data: 100%|██████████| 20.5M/20.5M [00:00<00:00, 28.2MB/s]\n",
      "Downloading data: 100%|██████████| 42.0M/42.0M [00:01<00:00, 31.9MB/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a78edd6cbc9441faba60c1ad9c63bbf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8731f76eee59480cac0dc67141d7ec1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2770c67f12f04b61bbf1701bb9b609db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a426d668",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # GPT-2 has no pad token by default\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9ac9c3d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "\n",
    "model.config.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d60cf1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"text\"], padding = \"max_length\", truncation = True, max_length = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5176b07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding \n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a95235f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = ds[\"train\"]\n",
    "ds_test = ds[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "720c720a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f64426d6d3324334afaf422504b679da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_test = ds_test.map(tokenize_function, batched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e86f0ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_test = tokenized_test.rename_column(\"label\", \"labels\")  # Trainer expects \"labels\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0d6be9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'labels', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 25000\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3f0b4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary')\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8bf8d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_eval_batch_size=16,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Create Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b464f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1563' max='1563' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1563/1563 14:34]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_results = trainer.evaluate(eval_dataset=tokenized_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d31de1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base GPT-2 performance without fine-tuning:\n",
      "Accuracy: 0.5004\n",
      "Precision: 0.5625\n",
      "Recall: 0.0036\n",
      "F1 Score: 0.0072\n"
     ]
    }
   ],
   "source": [
    "print(\"Base GPT-2 performance without fine-tuning:\")\n",
    "print(f\"Accuracy: {eval_results['eval_accuracy']:.4f}\")\n",
    "print(f\"Precision: {eval_results['eval_precision']:.4f}\")\n",
    "print(f\"Recall: {eval_results['eval_recall']:.4f}\")\n",
    "print(f\"F1 Score: {eval_results['eval_f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "TODO: In the cells below, we create a PEFT model from loaded model, run a training loop, and save the PEFT model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5775fadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig,TaskType\n",
    "lora_config = LoraConfig(\n",
    "    r=8,                       # Rank\n",
    "    lora_alpha=32,             # Alpha parameter for LoRA scaling\n",
    "    target_modules=[\"c_attn\", \"c_proj\"],  # Which modules to apply LoRA\n",
    "    lora_dropout=0.1,          # Dropout probability for LoRA layers\n",
    "    bias=\"none\",               # Bias configuration\n",
    "    task_type=TaskType.SEQ_CLS # Sequence classification task\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894046c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4d4c908",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/peft/tuners/lora.py:475: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters:  <bound method PeftModel.print_trainable_parameters of PeftModelForSequenceClassification(\n",
      "  (base_model): LoraModel(\n",
      "    (model): GPT2ForSequenceClassification(\n",
      "      (transformer): GPT2Model(\n",
      "        (wte): Embedding(50257, 768)\n",
      "        (wpe): Embedding(1024, 768)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "        (h): ModuleList(\n",
      "          (0-11): 12 x GPT2Block(\n",
      "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): GPT2Attention(\n",
      "              (c_attn): Linear(\n",
      "                in_features=768, out_features=2304, bias=True\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=2304, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (c_proj): Linear(\n",
      "                in_features=768, out_features=768, bias=True\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=768, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): GPT2MLP(\n",
      "              (c_fc): Conv1D()\n",
      "              (c_proj): Linear(\n",
      "                in_features=3072, out_features=768, bias=True\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=768, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (act): NewGELUActivation()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (score): ModulesToSaveWrapper(\n",
      "        (original_module): Linear(in_features=768, out_features=2, bias=False)\n",
      "        (modules_to_save): ModuleDict(\n",
      "          (default): Linear(in_features=768, out_features=2, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "from peft import get_peft_model\n",
    "lora_model = get_peft_model(model, lora_config)\n",
    "\n",
    "print(\"Trainable parameters: \", lora_model.print_trainable_parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4b5ead7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90d05a060378463a89a3c0afd578fcfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#ds_train = ds[\"train\"]\n",
    "tokenized_train = ds_train.map(tokenize_function, batched = True)\n",
    "tokenized_train = tokenized_train.rename_column(\"label\", \"labels\")  # Trainer expects \"labels\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4212fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results/gpt2-lora-imdb\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "97f6e64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f2873fc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='781' max='781' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [781/781 32:49, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.470700</td>\n",
       "      <td>0.425818</td>\n",
       "      <td>0.825440</td>\n",
       "      <td>0.809099</td>\n",
       "      <td>0.892664</td>\n",
       "      <td>0.739840</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=781, training_loss=1.0957841665582986, metrics={'train_runtime': 1971.7203, 'train_samples_per_second': 12.679, 'train_steps_per_second': 0.396, 'total_flos': 6592711800913920.0, 'train_loss': 1.0957841665582986, 'epoch': 1.0})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a7b4e1d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available: Tesla T4\n",
      "Current CUDA device index: 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU is available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Current CUDA device index: {torch.cuda.current_device()}\")\n",
    "else:\n",
    "    print(\"GPU is not available. Training will run on CPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b489c280",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fbeb05c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model.save_pretrained(\"lora-gpt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e8a663",
   "metadata": {},
   "source": [
    "###  ⚠️ IMPORTANT ⚠️\n",
    "HINTS!\n",
    "Due to cloud workspace storage constraints, you should not store the model weights in the same directory but rather use `/tmp` to avoid workspace crashes which are irrecoverable.\n",
    "Ensure to save it in /tmp always."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fa7fe003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "#lora_model.save(\"/tmp/your_model_name\")\n",
    "lora_model.save_pretrained(\"/tmp/lora-gpt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "TODO: In the cells below, we load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Comparing the results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bc3a8147",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "866ab28c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f9a32e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_path = \"/tmp/lora-gpt\"\n",
    "base_model_name = \"gpt2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7ac9d428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PEFT model from: /tmp/lora-gpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading PEFT model from: {adapter_path}\")\n",
    "loaded_lora_model = AutoPeftModelForSequenceClassification.from_pretrained(adapter_path)\n",
    "loaded_lora_model.to(device) # Move model to GPU if available\n",
    "print(\"Model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ce9dab83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "eval_args = TrainingArguments(\n",
    "    output_dir=\"./results/lora_eval\", \n",
    "    per_device_eval_batch_size=1,  \n",
    "    report_to=\"none\"               \n",
    ")\n",
    "\n",
    "lora_eval_trainer = Trainer(\n",
    "    model=loaded_lora_model,        \n",
    "    args=eval_args,\n",
    "    compute_metrics=compute_metrics, \n",
    "    tokenizer=tokenizer            \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "099d3e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating the fine-tuned LoRA model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25000' max='25000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25000/25000 15:16]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"\\nEvaluating the fine-tuned LoRA model...\")\n",
    "lora_eval_results = lora_eval_trainer.evaluate(eval_dataset=tokenized_test) # Your test data from cell [10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bb83ac4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base GPT-2 performance without fine-tuning:\n",
      "Accuracy: 0.5004\n",
      "Precision: 0.5625\n",
      "Recall: 0.0036\n",
      "F1 Score: 0.0072\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LoRA GPT2 performance with PEFT:\n",
      "Accuracy: 0.5249\n",
      "Precision: 0.8228\n",
      "Recall: 0.0635\n",
      "F1 Score: 0.1179\n"
     ]
    }
   ],
   "source": [
    "print(\"Base GPT-2 performance without fine-tuning:\")\n",
    "print(f\"Accuracy: {eval_results['eval_accuracy']:.4f}\")\n",
    "print(f\"Precision: {eval_results['eval_precision']:.4f}\")\n",
    "print(f\"Recall: {eval_results['eval_recall']:.4f}\")\n",
    "print(f\"F1 Score: {eval_results['eval_f1']:.4f}\")\n",
    "\n",
    "print(\"\\n\\n\\n\")\n",
    "\n",
    "print(\"LoRA GPT2 performance with PEFT:\")\n",
    "print(f\"Accuracy: {lora_eval_results['eval_accuracy']:.4f}\")\n",
    "print(f\"Precision: {lora_eval_results['eval_precision']:.4f}\")\n",
    "print(f\"Recall: {lora_eval_results['eval_recall']:.4f}\")\n",
    "print(f\"F1 Score: {lora_eval_results['eval_f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9ac32c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
